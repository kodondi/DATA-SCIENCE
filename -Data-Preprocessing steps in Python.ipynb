{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing steps in Python for any Machine Learning Algorithm\n",
    "\n",
    "#Data Preparation is one of the indispensable steps in any Machine Learning development life cycle. In today’s world, the data is present in a structured as well as unstructured form. To deal with such data, data scientists spent almost 70–80% of their time in preparing data for further analysis which includes:\n",
    "Handling missing values\n",
    "Encoding string values to integer values\n",
    "Split data into a train-test-validation dataset\n",
    "Feature scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Name Hire Date   Salary  Sick Days remaining\n",
      "0  Graham Chapman  03/15/14  50000.0                   10\n",
      "1     John Cleese  06/01/15  65000.0                    8\n",
      "2       Eric Idle  05/12/14  45000.0                   10\n",
      "3     Terry Jones  11/01/13  70000.0                    3\n",
      "4   Terry Gilliam  08/12/14  48000.0                    7\n",
      "5   Michael Palin  05/23/13  66000.0                    8\n"
     ]
    }
   ],
   "source": [
    "#Handling missing values\n",
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Importing dataset\n",
    "\n",
    "#df = pd.read_csv(r\"C:\\Users\\trainer\\Dropbox\\My PC (trainer-PC)\\Desktop\\ROOT DIRECTORY\\DATA SCIENCE\\csv files\\hrdata1.csv\",sep=' ' )\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\trainer\\Dropbox\\My PC (trainer-PC)\\Desktop\\ROOT DIRECTORY\\DATA SCIENCE\\csv files\\hrdata1.csv\" )\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Employee   Age   Salary Purchased\n",
      "0   Anjali  45.0  71000.0        No\n",
      "1    Parul  28.0  48000.0       Yes\n",
      "2  Kanisha  31.0  53000.0        No\n",
      "3    Parul  35.0  61000.0        No\n",
      "4  Kanisha  42.0      NaN       Yes\n",
      "5   Anjali  35.0  59000.0       Yes\n",
      "6    Parul   NaN  53000.0        No\n",
      "7   Anjali  47.0  80000.0       Yes\n",
      "8  Kanisha  51.0  81000.0        No\n",
      "9   Anjali  36.0  68000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "#Handling missing values\n",
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Importing dataset\n",
    "df = pandas.read_csv(r\"C:\\Users\\trainer\\Dropbox\\My PC (trainer-PC)\\Desktop\\raw2.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As you can see, we have 2 cells with missing values in the above image at row #4 and #6. We can handle these missing values by replacing the values with the following aggregate values:\n",
    "Mean\n",
    "-Mode (most frequent)\n",
    "-Median\n",
    "-Constant value\n",
    "-Delete the entire row/column with missing values\n",
    "There is no rule of thumb to select a specific option, it depends on the data and the problem statement which is intended to solve. To select the best option, the knowledge of both data and the application are needed.\n",
    "Let’s separate the independent and the dependent variable before handing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Anjali' 45.0 71000.0]\n",
      " ['Parul' 28.0 48000.0]\n",
      " ['Kanisha' 31.0 53000.0]\n",
      " ['Parul' 35.0 61000.0]\n",
      " ['Kanisha' 42.0 nan]\n",
      " ['Anjali' 35.0 59000.0]\n",
      " ['Parul' nan 53000.0]\n",
      " ['Anjali' 47.0 80000.0]\n",
      " ['Kanisha' 51.0 81000.0]\n",
      " ['Anjali' 36.0 68000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,[0,1,2]].values\n",
    "y = df.iloc[:,3].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Anjali' 45.0 71000.0]\n",
      " ['Parul' 28.0 48000.0]\n",
      " ['Kanisha' 31.0 53000.0]\n",
      " ['Parul' 35.0 61000.0]\n",
      " ['Kanisha' 42.0 63777.77777777778]\n",
      " ['Anjali' 35.0 59000.0]\n",
      " ['Parul' 38.888888888888886 53000.0]\n",
      " ['Anjali' 47.0 80000.0]\n",
      " ['Kanisha' 51.0 81000.0]\n",
      " ['Anjali' 36.0 68000.0]]\n"
     ]
    }
   ],
   "source": [
    "#We use the SimpleImputer class from the sklearn.impute library to replace \n",
    "#the missing values with the mean value of the corresponding columns.\n",
    "\n",
    "#Handling missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan,strategy=\"mean\")\n",
    "imputer.fit(X[:,1:3])\n",
    "X[:,1:3] = imputer.transform(X[:,1:3])\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We replaced the missing values to 38.88 and 63777.77 respectively. Instead of mean, we can also replace values with their corresponding mode, median, or any constant values by\n",
    "changing the ‘strategy’ parameter.\n",
    "\n",
    "\n",
    "#Encoding dataset features\n",
    "It is obvious to have a string-based column in the dataset in\n",
    "the form of names, addresses, and so on.\n",
    "But no machine learning algorithm can train the model with string-based variables in it.\n",
    "Hence, we have to encode those variables into numeric-based variables before delving into any machine learning algorithm.\n",
    "There are several ways by which we can handle such kind of data. Here,\n",
    "we will use 2 special Python libraries to convert the string-based into numeric-based variables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 45.0 71000.0]\n",
      " [0.0 0.0 1.0 28.0 48000.0]\n",
      " [0.0 1.0 0.0 31.0 53000.0]\n",
      " [0.0 0.0 1.0 35.0 61000.0]\n",
      " [0.0 1.0 0.0 42.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 59000.0]\n",
      " [0.0 0.0 1.0 38.888888888888886 53000.0]\n",
      " [1.0 0.0 0.0 47.0 80000.0]\n",
      " [0.0 1.0 0.0 51.0 81000.0]\n",
      " [1.0 0.0 0.0 36.0 68000.0]]\n"
     ]
    }
   ],
   "source": [
    "#encoding the independent variables\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "X = np.array(X)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are a lot of things happening in the above program. First, as you can see, we have 3 unique values in the Employee column. \n",
    "#The above code replaced the values with numeric values as shown below.\n",
    "\n",
    "The Machine Learning algorithm has nothing to do with the column names, instead,\n",
    "it tries to find the patterns within the data. As per Figure 7,\n",
    "we can easily infer that the value 2 is bigger than the value 1 and 0; it may also infer that there is a numerical order within the data. Hence,\n",
    "some misinterpretation could happen between independent variables and the dependent variables which could lead to the wrong correlations and future results.\n",
    "To mitigate such issues, we will create 1 column for each value with 0 and 1; 0 being the value is absent and 1 being present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "If you noticed, the dependent variable is also string-based. \n",
    "But, the dependent column has only 2 unique values;\n",
    "in that case, we can skip the above process, and directly we can encode the values from [‘No’, ‘Yes’] to [0,1] correspondingly. After encoding, you will notice that we have \n",
    "only 0 and 1 in the dependent column, and that is what we want.\n",
    "Let’s quickly modify the dependent variable ‘y’ to a numerical-based variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#encoding the dependent variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split data into a train-test-validation dataset\n",
    "In every model development process, we need to train the model with a subset of the original data, further, the model predicts the values for the new unseen data to evaluate its performance in terms of accuracy, ROC-AUC curve, and so on. We will not be going to discuss any evaluation parameters here as it is out of scope for this tutorial,\n",
    "but I cover those in my further tutorial for sure, until then stay tuned. :)\n",
    "To achieve the above scenario, we need to split the original dataset into 2 or sometimes 3 splits namely training, validation and testing dataset. Let’s discuss all the 3 datasets and their significance in the machine learning life cycle.\n",
    "Training dataset: It consists of more than half of the original dataset, the sole purpose of this dataset it to train the model and update the weights of the model.\n",
    "Validation dataset: A small subset of original data is used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. It is optional to have a validation dataset into the model.\n",
    "Testing dataset: It ranges from 10–25% of the original data to evaluate the model performance based on various evaluation parameters discussed above.\n",
    "Generally, it is recommended to have a split of 70–30 or 80–20 ratios of the train-test split, and 60–20–20 or 70–15–15 in case of the train-validation-split dataset.\n",
    "Consider the below example with 100 rows in the original dataset. In the middle split, an 80–20 split ratio happened between training and testing dataset. The training dataset further split into a 75–25 split ratios between training and validation dataset in the last split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (8, 5)\n",
      "X_test.shape:   (2, 5)\n",
      "y_train.shape:  (8,)\n",
      "y_test.shape:   (2,)\n"
     ]
    }
   ],
   "source": [
    "#split data into train and test dataset\n",
    "from sklearn.model_selection import train_test_split  #(for python2)\n",
    "#from sklearn.model_selection import train_test_split  (for python3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('X_test.shape:  ', X_test.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_test.shape:  ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature scaling\n",
    "\n",
    "Why Feature scaling?\n",
    "It is a technique to standardize the independent variables into a fixed range. It is a very crucial step in the process of data preparation because if we skip this step, the distance-based models cause variables with larger values to tend to dominate the variables with smaller values.\n",
    "We can achieve this in various ways, but we will discuss here the 2 most popular feature scaling techniques i.e. Min-Max scaling and Standardization. Let’s discuss it one by one.\n",
    "Min-Max scaling: In this technique, the features/variables are re-scaled between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          2.64575131 -0.77459667  0.62900131  0.08470159]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.40813062  0.52865477]\n",
      " [-1.         -0.37796447  1.29099445 -1.7909732  -1.57428135]\n",
      " [-1.         -0.37796447  1.29099445  0.0912292  -1.04854732]\n",
      " [ 1.         -0.37796447 -0.77459667  1.49327793  1.79041645]\n",
      " [-1.         -0.37796447  1.29099445 -0.58098595 -0.20737287]\n",
      " [ 1.         -0.37796447 -0.77459667  1.14756728  0.84409519]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.58098595 -0.41766648]]\n",
      "[[-1.          2.64575131 -0.77459667 -1.27240724 -1.04854732]\n",
      " [-1.          2.64575131 -0.77459667  2.18469922  1.89556325]]\n"
     ]
    }
   ],
   "source": [
    "#In our case, we will apply the standardization technique to scale the independent features in the training and testing dataset using sklearn.\n",
    "#preprocessing library.\n",
    "#feature scaling of training dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here if you noticed, I used the fit_transform method for the training dataset and transform method to the testing dataset, the reason being that we learn the scaling parameter from the training dataset, and used the same parameters to scale the testing dataset.\n",
    "Note: In our case, we do not apply feature scaling on the dependent variable as the dependent variable is already within the required range values.\n",
    "Now, the big question arises, where to apply feature scaling or where to not. A rule of thumb says, apply feature scaling on distance-based models such as KNN, PCA, etc. It further helps in speed-up the model training and prediction time. We can skip feature scaling in case of the tree-based models such as Decision Tree or probability-based models such as Naive Bayes because in these models the weight assigned according to the values present in the data.\n",
    "Conclusion\n",
    "Data Preparation is very crucial as well as the time-consuming process in any model development life-cycle. All the steps must be carried out carefully as every step discussed above somehow may affect the model performance if not implemented correctly.\n",
    "I hope all the concepts related to the data preparation has been cleared now. Try implementing the above steps with a real-time dataset. Please reach out in the below comment section if you have any doubt. Find the complete code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
